# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0

AWSTemplateFormatVersion: 2010-09-09
Description: >-
  batch-protein-folding-cfn-download.yaml: Download data to FSx for Lustre.

Parameters:
  ApplicationName:
    Description: Name of the application, if applicable
    Type: String
    Default: "Unknown"
  DownloadJobQueue:
    Description: Batch job queue for running download job.
    Type: String  
  JobDefinition:
    Description: Batch job definition for running download job.
    Type: String

Resources:
  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - "sts:AssumeRole"
      Policies:
        - PolicyName: SubmitJob
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action: batch:SubmitJob
                Resource: "*"
      ManagedPolicyArns:
        - "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
        - "arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
        - "arn:aws:iam::aws:policy/service-role/AWSLambdaVPCAccessExecutionRole"

  DownloadFsxDataLambda:
    Type: AWS::Lambda::Function
    Properties:
      ReservedConcurrentExecutions: 1
      Code: 
        ZipFile: |
          import boto3
          import cfnresponse
          import logging
          import pathlib

          LOGGER = logging.getLogger()
          LOGGER.setLevel(logging.INFO)
          boto_session = boto3.session.Session()
          region = boto_session.region_name
          batch = boto_session.client("batch", region_name=region)

          def lambda_handler(event, context):
              try:
                  LOGGER.info("REQUEST RECEIVED:\n %s", event)
                  LOGGER.info("REQUEST RECEIVED:\n %s", context)
                  if event["RequestType"] == "Create":
                      LOGGER.info("CREATE!")

                      download_scripts = [
                          "/scripts/download_alphafold_params.sh",
                          # "/scripts/download_bfd.sh",
                          # "/scripts/download_esmfold_params.sh",
                          # "/scripts/download_mgnify.sh",
                          # "/scripts/download_openfold_params.sh",
                          # "/scripts/download_pdb_mmcif.sh",
                          # "/scripts/download_pdb_seqres.sh",
                          # "/scripts/download_pdb70.sh",
                          # "/scripts/download_small_bfd.sh",
                          # "/scripts/download_uniref30.sh",
                          # "/scripts/download_uniprot.sh",                          
                          # "/scripts/download_uniref90.sh",
                          # "/scripts/download_omegafold_params.sh",
                          # "/scripts/download_rfdesign_params.sh",
                          # "/scripts/download_diffdock_params.sh",
                          "/scripts/download_rfdiffusion_params.sh"
                      ]
                      
                      responses = []
                      for script in download_scripts:
                          LOGGER.info(script)
                          script_response = submit_download_data_job(
                              job_queue=event["ResourceProperties"]["DownloadJobQueue"],
                              job_definition=event["ResourceProperties"]["JobDefinition"],
                              job_name=pathlib.Path(script).stem,
                              script=script,
                              cpu=4,
                              memory=16,
                              download_dir="/database",
                          )
                          LOGGER.info(f"Job ID {script_response.get('jobId', [])} submitted")
                          responses.append(script_response)
                      cfnresponse.send(
                          event, context, cfnresponse.SUCCESS, {"response": "Resource creation successful!"}
                      )

                  elif event["RequestType"] == "Update":
                      LOGGER.info("UPDATE!")
                      cfnresponse.send(
                          event,
                          context,
                          cfnresponse.SUCCESS,
                          {"response": "Resource update successful!"},
                      )
                  elif event["RequestType"] == "Delete":
                      LOGGER.info("DELETE!")
                      cfnresponse.send(
                          event,
                          context,
                          cfnresponse.SUCCESS,
                          {"response": "Resource deletion successful!"},
                      )
                  else:
                      LOGGER.info("FAILED!")
                      cfnresponse.send(
                          event,
                          context,
                          "FAILED",
                          {"response": "Unexpected event received from CloudFormation"},
                      )
              except:
                  LOGGER.info("FAILED!")
                  cfnresponse.send(
                      event,
                      context,
                      "FAILED",
                      {"response": "Exception during processing"},
                  )

          def submit_download_data_job(
            job_definition,
            job_queue,
            job_name="download_job",
            script="all",
            cpu=4,
            memory=16,
            download_dir="/database",
            depends_on=None
          ):
            container_overrides = {
                "command": [f"bash {script} {download_dir}"],
                "resourceRequirements": [
                    {"value": str(cpu), "type": "VCPU"},
                    {"value": str(memory * 1000), "type": "MEMORY"},
                ],
            }
            LOGGER.info(f"Job definition is {job_definition}")
            LOGGER.info(f"Job name is {job_name}")
            LOGGER.info(f"Job queue is {job_queue}")
            LOGGER.info(f"Container overrides are {container_overrides}")
            if depends_on:
              response = batch.submit_job(
                  jobDefinition=job_definition,
                  jobName=job_name,
                  jobQueue=job_queue,
                  containerOverrides=container_overrides,
                  dependsOn=[{"jobId":job_id, "type":"Sequential"} for job_id in depends_on]  
              )
            else:
              response = batch.submit_job(
                  jobDefinition=job_definition,
                  jobName=job_name,
                  jobQueue=job_queue,
                  containerOverrides=container_overrides
              )
            LOGGER.info(f"Response is {response}")
            return response         
                
      Description: Download data to FSx
      Handler: index.lambda_handler
      MemorySize: 512
      Role:
        Fn::GetAtt: LambdaExecutionRole.Arn
      Runtime: python3.10
      Timeout: 10
      Tags:
        - Key: Application
          Value: !Ref ApplicationName
        - Key: StackId
          Value: !Ref AWS::StackId

  DownloadFsxData:
    Type: Custom::ResourceForDownloadingData
    Properties:
      ServiceToken:
        Fn::GetAtt: DownloadFsxDataLambda.Arn
      DownloadJobQueue:
        Ref: DownloadJobQueue  
      JobDefinition:
        Ref: JobDefinition